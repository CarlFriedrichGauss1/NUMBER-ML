{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "x_train = train_dataset.data.float() / 255  #κανονικοποιούμε τις τιμές των pixel\n",
    "y_train = train_dataset.targets\n",
    "\n",
    "x_test = test_dataset.data.float() / 255\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "\n",
    "#W_1 = np.random.rand(784,128) - 0.5\n",
    "b_1 = np.random.rand(256, 1) - 0.5 \n",
    "\n",
    "#W_2 = np.random.rand(128, 128) - 0.5 \n",
    "b_2 = np.random.rand(128, 1) - 0.5\n",
    "\n",
    "#W_3 = np.random.rand(128, 10) - 0.5\n",
    "b_3 = np.random.rand(10, 1) - 0.5 #ορίζουμε τυχαία τους πίνακς των biases και weights με τις απαιτούμενες διαστάσεις\n",
    "\n",
    "W_1 = np.random.randn(784, 256) * np.sqrt(2.0 / 784)\n",
    "W_2 = np.random.randn(256, 128) * np.sqrt(2.0 / 128)\n",
    "W_3 = np.random.randn(128, 10) * np.sqrt(2.0 / 128)\n",
    "\n",
    "A_layers = []\n",
    "for i in range(len(x_train)):\n",
    "   A_layers.append(x_train[i].reshape(784, 1))\n",
    "\n",
    "def ReLU(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def softmax(Z):\n",
    "  Z = Z - np.max(Z)  \n",
    "  exp_values = np.exp(Z)\n",
    "  sum_exp_values = np.sum(exp_values)     #απλά εφαρμόζουμε τον τύπο της softmax και κάνουμε τις τιμές των 10 νευρώνων μία κατανομή διακριτής τυχαίας μεταβλητής\n",
    "  A_out_prob = exp_values / sum_exp_values\n",
    "  return A_out_prob\n",
    "\n",
    "\n",
    "def forwardProp(A , W_1 , b_1, W_2, b_2, W_3 , b_3): #με A να είναι ένας 784x1 πινακας η ίσοδος layer 0 στο νευρωνικό δύκτιο , W να έιναι ένας\n",
    "                                          #784x10 πίνακας με τα βάρη των ακμών και b να είναι ένας 10x1 πίνακας τα biases όλων των νέων κορυφών\n",
    "  A_1 = np.dot(W_1.T , A)\n",
    "  Z_1 = A_1 + b_1\n",
    "  A_1 = ReLU(Z_1)\n",
    "  A_2 = np.dot(W_2.T, A_1)\n",
    "  Z_2 = A_2 + b_2\n",
    "  A_2 = ReLU(Z_2)\n",
    "  A_3 = np.dot(W_3.T , A_2)\n",
    "  Z_3 = A_3 + b_3\n",
    "  A_3 = softmax(Z_3)\n",
    "\n",
    "  return A_1, Z_1, A_2 , Z_2 , A_3 , Z_3\n",
    "\n",
    "def d_ReLU(x):\n",
    "  return x > 0\n",
    "\n",
    "\n",
    "def backProp(A, A_1, A_2, A_3, Z_1, Z_2, Z_3, W_1, W_2 ,W_3 , mean):\n",
    "  vector = np.zeros((10, 1))\n",
    "  vector[mean, 0] = 1\n",
    "\n",
    "  dZ3 = (A_3 - vector)\n",
    "  dW3 = np.dot(A_2, dZ3.T)\n",
    "  db3 = dZ3\n",
    "\n",
    "  dA2 = np.dot(W_3, dZ3)\n",
    "  dZ2 = dA2 * d_ReLU(Z_2)\n",
    "  dW2 = np.dot(A_1, dZ2.T)\n",
    "  db2 = dZ2\n",
    "\n",
    "  dA1 = np.dot(W_2, dZ2)\n",
    "  dZ1 = dA1 * d_ReLU(Z_1)\n",
    "  dW1 = np.dot(A, dZ1.T)\n",
    "  db1 = dZ1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3 , db3\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def gradientDescent(W_1, b_1, W_2, b_2, W_3, b_3, learning_rate, epochs):\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "      correct_predictions = 0\n",
    "      total_samples = len(x_train)\n",
    "      for i in range(total_samples):\n",
    "          A = A_layers[i]\n",
    "          A_1, Z_1, A_2 , Z_2, A_3, Z_3 = forwardProp(A, W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "\n",
    "          predicted_label = np.argmax(A_3)\n",
    "          if predicted_label == y_train[i]:\n",
    "              correct_predictions += 1\n",
    "\n",
    "          dW_1, db_1, dW_2, db_2 , dW_3, db_3 = backProp(A, A_1, A_2,A_3, Z_1, Z_2,Z_3,  W_1, W_2, W_3 ,y_train[i])\n",
    "\n",
    "          W_1 = W_1 - learning_rate * dW_1\n",
    "          b_1 = b_1 - learning_rate * db_1\n",
    "          W_2 = W_2 - learning_rate * dW_2\n",
    "          b_2 = b_2 - learning_rate * db_2\n",
    "          W_3 = W_3 - learning_rate * dW_3\n",
    "          b_3 = b_3 - learning_rate * db_3\n",
    "        \n",
    "      accuracy = (correct_predictions / total_samples) * 100\n",
    "      accuracy_list.append(accuracy)\n",
    "      print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        learning_rate /= 100\n",
    "        print(f\"Learning rate decayed to {learning_rate}\")\n",
    "\n",
    "    return accuracy_list[-1], W_1, b_1, W_2, b_2 , W_3, b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/0r5f_r2d4b19rx1kcvn6hym80000gn/T/ipykernel_91504/3052920989.py:48: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  A_1 = np.dot(W_1.T , A)\n",
      "/var/folders/6f/0r5f_r2d4b19rx1kcvn6hym80000gn/T/ipykernel_91504/3052920989.py:79: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  dW1 = np.dot(A, dZ1.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 81.38%\n",
      "Epoch 2, Accuracy: 85.63%\n",
      "Epoch 3, Accuracy: 86.87%\n",
      "Epoch 4, Accuracy: 87.71%\n",
      "Epoch 5, Accuracy: 88.22%\n",
      "Learning rate decayed to 0.0001\n",
      "Epoch 6, Accuracy: 89.91%\n",
      "Epoch 7, Accuracy: 90.67%\n",
      "Epoch 8, Accuracy: 90.79%\n",
      "Epoch 9, Accuracy: 90.98%\n",
      "Epoch 10, Accuracy: 91.07%\n",
      "Learning rate decayed to 1e-06\n",
      "Epoch 11, Accuracy: 91.13%\n",
      "Epoch 12, Accuracy: 91.14%\n",
      "Epoch 13, Accuracy: 91.14%\n",
      "Epoch 14, Accuracy: 91.14%\n"
     ]
    }
   ],
   "source": [
    "accuracy, c_W_1, c_b_1, c_W_2, c_b_2 , c_W_3 , c_b_3 = gradientDescent(W_1, b_1, W_2, b_2, W_3, b_3, 0.01, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [61, 115, 124, 149, 151, 217, 241, 247, 266, 321]\n",
    "for i in indexes:\n",
    "    input_data = x_test[i].reshape(784, 1)\n",
    "    A_1, Z_1, A_2 , Z_2, A_3, A_3 = forwardProp(input_data, c_W_1, c_b_1 , c_W_2, c_b_2,c_W_3, c_b_3 )\n",
    "    predicted_label = np.argmax(A_3)\n",
    "    print(\"Label:\" , y_test[i])\n",
    "    print(\"Prediction:\", predicted_label)\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    plt.title(f\"MNIST Digit: {y_test[i]}\") \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
