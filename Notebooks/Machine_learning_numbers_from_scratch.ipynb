{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "x_train = train_dataset.data.float() / 255  #κανονικοποιούμε τις τιμές των pixel\n",
    "y_train = train_dataset.targets\n",
    "\n",
    "x_test = test_dataset.data.float() / 255\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "\n",
    "W_1 = np.random.rand(784,10) - 0.5\n",
    "b_1 = np.random.rand(10, 1) - 0.5 \n",
    "W_2 = np.random.rand(10, 10) - 0.5\n",
    "b_2 = np.random.rand(10, 1) - 0.5 #ορίζουμε τυχαία τους πίνακς των biases και weights με τις απαιτούμενες διαστάσεις\n",
    "A_layers = []\n",
    "for i in range(len(x_train)):\n",
    "   A_layers.append(x_train[i].reshape(784, 1))\n",
    "\n",
    "def ReLU(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def softmax(Z):\n",
    "  Z = Z - np.max(Z)  \n",
    "  exp_values = np.exp(Z)\n",
    "  sum_exp_values = np.sum(exp_values)     #απλά εφαρμόζουμε τον τύπο της softmax και κάνουμε τις τιμές των 10 νευρώνων μία κατανομή διακριτής τυχαίας μεταβλητής\n",
    "  A_out_prob = exp_values / sum_exp_values\n",
    "  return A_out_prob\n",
    "\n",
    "\n",
    "def forwardProp(A , W_1 , b_1, W_2, b_2): #με A να είναι ένας 784x1 πινακας η ίσοδος layer 0 στο νευρωνικό δύκτιο , W να έιναι ένας\n",
    "                                          #784x10 πίνακας με τα βάρη των ακμών και b να είναι ένας 10x1 πίνακας τα biases όλων των νέων κορυφών\n",
    "  A_1 = np.dot(W_1.T , A)\n",
    "  Z_1 = A_1 + b_1\n",
    "  A_1 = ReLU(Z_1)\n",
    "  A_2 = np.dot(W_2.T, A_1)\n",
    "  Z_2 = A_2 + b_2\n",
    "  A_2 = softmax(Z_2)\n",
    "  return A_1, Z_1, A_2 , Z_2\n",
    "\n",
    "def d_ReLU(x):\n",
    "  return x > 0\n",
    "\n",
    "\n",
    "def backProp(A, A_1, A_2, Z_1, Z_2, W_1, W_2, mean):\n",
    "    vector = np.zeros((10, 1))\n",
    "    vector[mean, 0] = 1\n",
    "\n",
    "    \n",
    "    dZ2 = 2 * (A_2 - vector) #10x1\n",
    "    dW2 = np.dot(A_1, dZ2.T)\n",
    "    db2 = dZ2\n",
    "\n",
    "    dA1 = np.dot(W_2, dZ2) #(10x10) x (10x1)\n",
    "    dZ1 = dA1 * d_ReLU(Z_1) #10x1\n",
    "    dW1 = np.dot(A, dZ1.T) #(784x1) x (1x10)  \n",
    "    db1 = dZ1\n",
    "    \n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def gradientDescent(W_1, b_1, W_2, b_2, learning_rate, epochs):\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "      correct_predictions = 0\n",
    "      total_samples = len(x_train)\n",
    "      for i in range(total_samples):\n",
    "          A = A_layers[i]\n",
    "          A_1, Z_1, A_2 , Z_2 = forwardProp(A, W_1, b_1, W_2, b_2)\n",
    "\n",
    "          predicted_label = np.argmax(A_2)\n",
    "          if predicted_label == y_train[i]:\n",
    "              correct_predictions += 1\n",
    "\n",
    "          dW_1, db_1, dW_2, db_2 = backProp(A, A_1, A_2, Z_1, Z_2, W_1, W_2 ,y_train[i])\n",
    "\n",
    "          W_1 = W_1 - learning_rate * dW_1\n",
    "          b_1 = b_1 - learning_rate * db_1\n",
    "          W_2 = W_2 - learning_rate * dW_2\n",
    "          b_2 = b_2 - learning_rate * db_2\n",
    "        \n",
    "      accuracy = (correct_predictions / total_samples) * 100\n",
    "      accuracy_list.append(accuracy)\n",
    "      print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        learning_rate *= 0.5\n",
    "        print(f\"Learning rate decayed to {learning_rate}\")\n",
    "\n",
    "    return accuracy_list[-1], W_1, b_1, W_2, b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/0r5f_r2d4b19rx1kcvn6hym80000gn/T/ipykernel_21220/3370617592.py:39: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  A_1 = np.dot(W_1.T , A)\n",
      "/var/folders/6f/0r5f_r2d4b19rx1kcvn6hym80000gn/T/ipykernel_21220/3370617592.py:62: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  dW1 = np.dot(A, dZ1.T) #A.dot(dZ_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 86.62%\n",
      "Epoch 2, Accuracy: 90.54%\n",
      "Epoch 3, Accuracy: 91.16%\n",
      "Epoch 4, Accuracy: 91.40%\n",
      "Epoch 5, Accuracy: 91.72%\n",
      "Learning rate decayed to 0.005\n",
      "Epoch 6, Accuracy: 93.29%\n",
      "Epoch 7, Accuracy: 93.59%\n",
      "Epoch 8, Accuracy: 93.67%\n"
     ]
    }
   ],
   "source": [
    "accuracy, c_W_1, c_b_1, c_W_2, c_b_2 = gradientDescent(W_1, b_1, W_2, b_2, 0.01, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_test_accuracy(W_1, b_1, W_2, b_2, x_data, y_data):\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(x_data)\n",
    "    wrong_index = []\n",
    "\n",
    "    for n in range(total_samples):\n",
    "        input_data = x_data[n].reshape(784, 1)\n",
    "        \n",
    "        A_1, Z_1, A_2, Z_2 = forwardProp(input_data, W_1, b_1, W_2, b_2)\n",
    "        \n",
    "        predicted_label = np.argmax(A_2)\n",
    "    \n",
    "        if predicted_label == y_data[n]:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            wrong_index.append(n)\n",
    "    \n",
    "    accuracy_1 = (correct_predictions / total_samples) * 100\n",
    "    \n",
    "    return accuracy_1, wrong_index\n",
    "\n",
    "\n",
    "test_accuracy, wrong = evaluate_test_accuracy(c_W_1, c_b_1, c_W_2, c_b_2, x_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {test_accuracy:.2f}%\")\n",
    "print(len(wrong))\n",
    "\n",
    "\n",
    "for i in range(3875,3885):\n",
    "    input_data = x_test[i].reshape(784, 1)\n",
    "    A_1, Z_1, A_2 , Z_2 = forwardProp(input_data, c_W_1, c_b_1 , c_W_2, c_b_2)\n",
    "    predicted_label = np.argmax(A_2)\n",
    "    print(\"Label:\" , y_test[i])\n",
    "    print(\"Prediction:\", predicted_label)\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    plt.title(f\"MNIST Digit: {y_test[i]}\") \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.2357, Train Acc: 0.9278, Val Loss: 0.0473, Val Acc: 0.9862\n",
      "Epoch 2/5, Train Loss: 0.1097, Train Acc: 0.9695, Val Loss: 0.0440, Val Acc: 0.9875\n",
      "Epoch 3/5, Train Loss: 0.0909, Train Acc: 0.9752, Val Loss: 0.0410, Val Acc: 0.9887\n",
      "Epoch 4/5, Train Loss: 0.0827, Train Acc: 0.9786, Val Loss: 0.0379, Val Acc: 0.9902\n",
      "Epoch 5/5, Train Loss: 0.0768, Train Acc: 0.9807, Val Loss: 0.0381, Val Acc: 0.9903\n",
      "Test Accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations with data augmentation for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),              # Rotate images by up to 10 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translations\n",
    "    transforms.ToTensor(),                      # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
    "])\n",
    "\n",
    "# Transformations for validation and test sets (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset_full = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Split training dataset into training (50,000) and validation (10,000) sets\n",
    "train_dataset = torch.utils.data.Subset(train_dataset_full, range(0, 50000))\n",
    "val_dataset = torch.utils.data.Subset(\n",
    "    torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_test),\n",
    "    range(50000, 60000)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the CNN model with batch normalization and dropout\n",
    "class AdvancedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        # Convolutional layers with batch normalization\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Reduce spatial dimensions\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.bn1(torch.relu(self.conv1(x))))  # Conv1 -> BN -> ReLU -> Pool\n",
    "        x = self.pool(self.bn2(torch.relu(self.conv2(x))))  # Conv2 -> BN -> ReLU -> Pool\n",
    "        x = self.bn3(torch.relu(self.conv3(x)))             # Conv3 -> BN -> ReLU\n",
    "        x = x.view(-1, 128 * 7 * 7)                        # Flatten\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))           # FC1 -> ReLU -> Dropout\n",
    "        x = self.dropout(torch.relu(self.fc2(x)))           # FC2 -> ReLU -> Dropout\n",
    "        x = self.fc3(x)                                     # FC3 (logits)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move to device\n",
    "model = AdvancedCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Adam with L2 regularization\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Accuracy computation function\n",
    "def accuracy(output, target):\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = (pred == target).sum().item()\n",
    "    return correct / target.size(0)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 5\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_acc += accuracy(outputs, labels) * inputs.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_acc += accuracy(outputs, labels) * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load the best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_acc += accuracy(outputs, labels) * inputs.size(0)\n",
    "\n",
    "test_acc /= len(test_loader.dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
