{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "x_train = train_dataset.data.float() / 255  #κανονικοποιούμε τις τιμές των pixel\n",
    "y_train = train_dataset.targets\n",
    "\n",
    "x_test = test_dataset.data.float() / 255\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "\n",
    "W_1 = np.random.rand(784,10) - 0.5\n",
    "b_1 = np.random.rand(10, 1) - 0.5 \n",
    "W_2 = np.random.rand(10, 10) - 0.5\n",
    "b_2 = np.random.rand(10, 1) - 0.5 #ορίζουμε τυχαία τους πίνακς των biases και weights με τις απαιτούμενες διαστάσεις\n",
    "A_layers = []\n",
    "for i in range(len(x_train)):\n",
    "   A_layers.append(x_train[i].reshape(784, 1))\n",
    "\n",
    "def ReLU(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def softmax(Z):\n",
    "  Z = Z - np.max(Z)  \n",
    "  exp_values = np.exp(Z)\n",
    "  sum_exp_values = np.sum(exp_values)     #απλά εφαρμόζουμε τον τύπο της softmax και κάνουμε τις τιμές των 10 νευρώνων μία κατανομή διακριτής τυχαίας μεταβλητής\n",
    "  A_out_prob = exp_values / sum_exp_values\n",
    "  return A_out_prob\n",
    "\n",
    "\n",
    "def forwardProp(A , W_1 , b_1, W_2, b_2): #με A να είναι ένας 784x1 πινακας η ίσοδος layer 0 στο νευρωνικό δύκτιο , W να έιναι ένας\n",
    "                                          #784x10 πίνακας με τα βάρη των ακμών και b να είναι ένας 10x1 πίνακας τα biases όλων των νέων κορυφών\n",
    "  A_1 = np.dot(W_1.T , A)\n",
    "  Z_1 = A_1 + b_1\n",
    "  A_1 = ReLU(Z_1)\n",
    "  A_2 = np.dot(W_2.T, A_1)\n",
    "  Z_2 = A_2 + b_2\n",
    "  A_2 = softmax(Z_2)\n",
    "  return A_1, Z_1, A_2 , Z_2\n",
    "\n",
    "def d_ReLU(x):\n",
    "  return x > 0\n",
    "\n",
    "\n",
    "def backProp(A, A_1, A_2, Z_1, Z_2, W_1, W_2, mean):\n",
    "    vector = np.zeros((10, 1))\n",
    "    vector[mean, 0] = 1\n",
    "\n",
    "    \n",
    "    dZ2 = 2 * (A_2 - vector) #10x1\n",
    "    dW2 = np.dot(A_1, dZ2.T)\n",
    "    db2 = dZ2\n",
    "\n",
    "    dA1 = np.dot(W_2, dZ2) #(10x10) x (10x1)\n",
    "    dZ1 = dA1 * d_ReLU(Z_1) #10x1\n",
    "    dW1 = np.dot(A, dZ1.T) #(784x1) x (1x10)  \n",
    "    db1 = dZ1\n",
    "    \n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def gradientDescent(W_1, b_1, W_2, b_2, learning_rate, epochs):\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "      correct_predictions = 0\n",
    "      total_samples = len(x_train)\n",
    "      for i in range(total_samples):\n",
    "          A = A_layers[i]\n",
    "          A_1, Z_1, A_2 , Z_2 = forwardProp(A, W_1, b_1, W_2, b_2)\n",
    "\n",
    "          predicted_label = np.argmax(A_2)\n",
    "          if predicted_label == y_train[i]:\n",
    "              correct_predictions += 1\n",
    "\n",
    "          dW_1, db_1, dW_2, db_2 = backProp(A, A_1, A_2, Z_1, Z_2, W_1, W_2 ,y_train[i])\n",
    "\n",
    "          W_1 = W_1 - learning_rate * dW_1\n",
    "          b_1 = b_1 - learning_rate * db_1\n",
    "          W_2 = W_2 - learning_rate * dW_2\n",
    "          b_2 = b_2 - learning_rate * db_2\n",
    "        \n",
    "      accuracy = (correct_predictions / total_samples) * 100\n",
    "      accuracy_list.append(accuracy)\n",
    "      print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        learning_rate *= 0.5\n",
    "        print(f\"Learning rate decayed to {learning_rate}\")\n",
    "\n",
    "    return accuracy_list[-1], W_1, b_1, W_2, b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/0r5f_r2d4b19rx1kcvn6hym80000gn/T/ipykernel_21220/3370617592.py:39: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  A_1 = np.dot(W_1.T , A)\n",
      "/var/folders/6f/0r5f_r2d4b19rx1kcvn6hym80000gn/T/ipykernel_21220/3370617592.py:62: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  dW1 = np.dot(A, dZ1.T) #A.dot(dZ_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 86.62%\n",
      "Epoch 2, Accuracy: 90.54%\n",
      "Epoch 3, Accuracy: 91.16%\n",
      "Epoch 4, Accuracy: 91.40%\n",
      "Epoch 5, Accuracy: 91.72%\n",
      "Learning rate decayed to 0.005\n",
      "Epoch 6, Accuracy: 93.29%\n",
      "Epoch 7, Accuracy: 93.59%\n",
      "Epoch 8, Accuracy: 93.67%\n"
     ]
    }
   ],
   "source": [
    "accuracy, c_W_1, c_b_1, c_W_2, c_b_2 = gradientDescent(W_1, b_1, W_2, b_2, 0.01, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_test_accuracy(W_1, b_1, W_2, b_2, x_data, y_data):\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(x_data)\n",
    "    wrong_index = []\n",
    "\n",
    "    for n in range(total_samples):\n",
    "        input_data = x_data[n].reshape(784, 1)\n",
    "        \n",
    "        A_1, Z_1, A_2, Z_2 = forwardProp(input_data, W_1, b_1, W_2, b_2)\n",
    "        \n",
    "        predicted_label = np.argmax(A_2)\n",
    "    \n",
    "        if predicted_label == y_data[n]:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            wrong_index.append(n)\n",
    "    \n",
    "    accuracy_1 = (correct_predictions / total_samples) * 100\n",
    "    \n",
    "    return accuracy_1, wrong_index\n",
    "\n",
    "\n",
    "test_accuracy, wrong = evaluate_test_accuracy(c_W_1, c_b_1, c_W_2, c_b_2, x_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {test_accuracy:.2f}%\")\n",
    "print(len(wrong))\n",
    "\n",
    "\n",
    "for i in range(3875,3885):\n",
    "    input_data = x_test[i].reshape(784, 1)\n",
    "    A_1, Z_1, A_2 , Z_2 = forwardProp(input_data, c_W_1, c_b_1 , c_W_2, c_b_2)\n",
    "    predicted_label = np.argmax(A_2)\n",
    "    print(\"Label:\" , y_test[i])\n",
    "    print(\"Prediction:\", predicted_label)\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    plt.title(f\"MNIST Digit: {y_test[i]}\") \n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
